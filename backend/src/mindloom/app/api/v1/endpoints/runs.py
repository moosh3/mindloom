from fastapi import APIRouter, Depends, HTTPException, status
from typing import List, Dict, Optional, Any
import uuid
from datetime import datetime
import asyncio
import json # For serializing input_data
import os # For potential env vars

from kubernetes import client, config

from mindloom.app.models.run import Run, RunCreate, RunStatus
from mindloom.dependencies import get_current_user
from mindloom.app.models.user import User
from mindloom.services.agents import AgentService # Keep for potential validation
from mindloom.services.teams import TeamService     # Keep for potential validation
from mindloom.core.config import settings
from mindloom.db.session import get_async_db_session
from mindloom.services.runs import run_service # Import the service instance
from mindloom.app.schemas.run import Run as RunSchema # Import Pydantic schema

# --- Kubernetes Configuration --- 
# Load Kubernetes configuration
try:
    if os.getenv('KUBERNETES_SERVICE_HOST'):
        config.load_incluster_config()
        print("Loaded in-cluster Kubernetes config")
    else:
        config.load_kube_config()
        print("Loaded local Kube config")
    K8S_BATCH_V1_API = client.BatchV1Api()
    K8S_NAMESPACE = settings.KUBERNETES_NAMESPACE # Expect KUBERNETES_NAMESPACE in settings
    print(f"Kubernetes client initialized for namespace: {K8S_NAMESPACE}")
except Exception as e:
    print(f"Error loading Kubernetes config: {e}. Job creation will fail.")
    K8S_BATCH_V1_API = None
    K8S_NAMESPACE = 'default' # Fallback or raise?

router = APIRouter(dependencies=[Depends(get_current_user)])

@router.post("/", response_model=RunSchema, status_code=status.HTTP_202_ACCEPTED, tags=["Runs"])
async def create_run(
    run_in: RunCreate,
    current_user: User = Depends(get_current_user),
    db: AsyncSession = Depends(get_async_db_session) # Inject DB session
) -> Run:
    """
    Start a new run for an agent or team by launching a Kubernetes Job.
    """
    # Basic validation (can be enhanced later)
    if run_in.runnable_type not in ['agent', 'team']:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Invalid runnable_type. Must be 'agent' or 'team'.")

    # TODO: Validate runnable_id exists using AgentService/TeamService if needed

    # 1. Create Run record in DB with PENDING status
    try:
        db_run = await run_service.create_run(
            db=db,
            runnable_id=run_in.runnable_id,
            runnable_type=run_in.runnable_type,
            input_variables=run_in.input,
            # user_id=current_user.id # Pass user ID if available from auth
        )
        run_id = db_run.id # Use the ID generated by the database
        print(f"Created Run {run_id} in database with status PENDING.")
    except Exception as e:
         # Log the error appropriately
        print(f"Error creating run record in database: {e}")
        raise HTTPException(status_code=500, detail="Failed to create run record in database.")


    if not K8S_BATCH_V1_API:
        # Update status to FAILED if K8s client failed to init
        await run_service.update_run_status(
            db=db,
            run_id=run_id,
            status=RunStatus.FAILED,
            output_data={"error": "Kubernetes client not initialized. Cannot create job."}
        )
        print(f"Marked Run {run_id} as FAILED in database due to K8s client initialization error.")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Kubernetes client not available. Cannot schedule run."
        )

    # 2. Define Kubernetes Job
    job_name = f"mindloom-run-{run_id}"
    namespace = settings.KUBERNETES_NAMESPACE

    # Prepare environment variables for the executor pod
    env_vars = [
        client.V1EnvVar(name="RUN_ID", value=str(run_id)),
        client.V1EnvVar(name="RUNNABLE_TYPE", value=run_in.runnable_type),
        client.V1EnvVar(name="RUNNABLE_ID", value=str(run_in.runnable_id)),
        client.V1EnvVar(name="INPUT_DATA", value=json.dumps(run_in.input or {})),
        # Assuming DATABASE_URL and REDIS_URL are needed by the executor
        # These should ideally come from Secrets or a ConfigMap in a real setup
        client.V1EnvVar(name="DATABASE_URL", value=settings.DATABASE_URL.unicode_string()),
        client.V1EnvVar(name="REDIS_URL", value=settings.REDIS_URL or ""),
        # Add other necessary env vars (e.g., API keys via Secrets)
        # client.V1EnvVar(name="OPENAI_API_KEY", value_from=client.V1EnvVarSource(secret_key_ref=client.V1SecretKeySelector(name="mindloom-secrets", key="openai-api-key"))),
    ]

    # Define the container for the Job
    container = client.V1Container(
        name="run-executor",
        image=settings.KUBERNETES_EXECUTOR_IMAGE, # Use image from settings
        command=["python", "/app/mindloom/execution/run_executor.py"], # Command to run the script
        env=env_vars,
        image_pull_policy="IfNotPresent", # Or "Always" if using :latest tag
        # Add resource requests/limits
        # resources=client.V1ResourceRequirements(
        #     requests={"cpu": "100m", "memory": "256Mi"},
        #     limits={"cpu": "500m", "memory": "512Mi"},
        # ),
    )

    # Define the Pod template spec
    template = client.V1PodTemplateSpec(
        metadata=client.V1ObjectMeta(labels={"app": "mindloom-run-executor", "run_id": str(run_id)}),
        spec=client.V1PodSpec(
            restart_policy="Never", # Jobs should not restart pods on failure
            containers=[container],
            # Add imagePullSecrets if using a private registry
            # image_pull_secrets=[client.V1LocalObjectReference(name="my-registry-secret")]
            # Consider serviceAccountName if specific permissions are needed for the pod
            # service_account_name="mindloom-executor-sa"
        ),
    )

    # Define the Job spec
    job_spec = client.V1JobSpec(
        template=template,
        backoff_limit=1, # Number of retries before marking job as failed
        ttl_seconds_after_finished=3600 # Auto-cleanup finished jobs after 1 hour
    )

    # Define the Job object
    job = client.V1Job(
        api_version="batch/v1",
        kind="Job",
        metadata=client.V1ObjectMeta(name=job_name, labels={"app": "mindloom-run", "run_id": str(run_id)}),
        spec=job_spec,
    )

    # 3. Create the Kubernetes Job
    try:
        print(f"Creating Kubernetes Job '{job_name}' in namespace '{namespace}'...")
        api_response = K8S_BATCH_V1_API.create_namespaced_job(body=job, namespace=namespace)
        print(f"Kubernetes Job created successfully. Job status: {api_response.status}")
    except client.ApiException as e:
        print(f"Error creating Kubernetes Job: {e.status} - {e.reason}")
        print(f"Body: {e.body}")
        # Attempt to mark the DB run as FAILED if Job creation fails
        try:
            await run_service.update_run_status(
                db=db,
                run_id=run_id,
                status=RunStatus.FAILED,
                output_data={"error": f"Failed to create Kubernetes Job: {e.reason}"}
            )
            print(f"Marked Run {run_id} as FAILED in database due to Job creation error.")
        except Exception as db_err:
             print(f"Failed to mark Run {run_id} as FAILED after Job creation error: {db_err}")

        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to launch run execution job: {e.reason}",
        )
    except Exception as e:
         # Catch other potential errors during job creation setup
         print(f"Unexpected error preparing or creating Kubernetes Job: {e}")
         # Attempt to mark the DB run as FAILED
         try:
             await run_service.update_run_status(
                 db=db,
                 run_id=run_id,
                 status=RunStatus.FAILED,
                 output_data={"error": f"Unexpected error during Job creation: {e}"}
             )
             print(f"Marked Run {run_id} as FAILED in database due to unexpected Job creation error.")
         except Exception as db_err:
              print(f"Failed to mark Run {run_id} as FAILED after unexpected Job creation error: {db_err}")
         raise HTTPException(
             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
             detail="An unexpected error occurred while launching the run execution job.",
         )


    # Return the initial Run object (status is PENDING)
    # Convert ORM object to Pydantic schema for response
    return RunSchema.model_validate(db_run)


@router.get("/", response_model=List[RunSchema], tags=["Runs"])
async def read_runs(
    skip: int = 0,
    limit: int = 100,
    runnable_id: Optional[uuid.UUID] = None, # Optional filter by agent/team
    status: Optional[RunStatus] = None       # Optional filter by status
) -> List[Run]:
    """
    Retrieve a list of runs, with optional filtering.
    """
    runs_list = await run_service.get_runs(
        db=get_async_db_session(),
        skip=skip,
        limit=limit,
        runnable_id=runnable_id,
        status=status
    )

    # Apply sorting (e.g., newest first)
    runs_list.sort(key=lambda r: r.created_at, reverse=True)

    return runs_list


@router.get("/{run_id}", response_model=RunSchema, tags=["Runs"])
async def read_run(run_id: uuid.UUID) -> Run:
    """
    Retrieve a specific run by ID.
    """
    run = await run_service.get_run(db=get_async_db_session(), run_id=run_id)
    if run is None:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Run not found")
    return run

# Potential future endpoint: Cancel a run
# @router.post("/{run_id}/cancel", response_model=Run, tags=["Runs"])
# async def cancel_run(run_id: uuid.UUID) -> Run:
#     ...
